{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd01820ff4cd54b671249640c9af2c450fe03cf6585e8fcc7bf8ac2ef6f79448c27",
   "display_name": "Python 3.7.3 64-bit ('benso': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "#Clean the data to remove stop-words, punctuation, and emoticons (similar to last week's hw assignment)\n",
    "#Apply LDA and print out 10 topics\n",
    "#Chatbot\n",
    "#Clean the data as you did for LDA\n",
    "#Build a chatbot based on the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Opening the file\n",
    "f = open(\"C:/Users/benso/Desktop/Projects/Usable_AI_Code/Homework10/amazon_cells_labelled.txt\", \"r\")\n",
    "\n",
    "data =[]\n",
    "# Converting it to pandas dataframe\n",
    "for line in f:\n",
    "    review = line[:len(line) - 2]\n",
    "    sentiment = \"neg\" if line[len(line)-2] == \"0\" else \"pos\"\n",
    "    row = [review, sentiment]\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['reviews', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             reviews sentiment\n",
       "0                   way plug us unless go converter.       neg\n",
       "1                        good case, excellent value.       pos\n",
       "2                                     great jawbone.       pos\n",
       "3  tied charger conversations lasting 45 minutes....       neg\n",
       "4                                         mic great.       pos"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviews</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>way plug us unless go converter.</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>good case, excellent value.</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>great jawbone.</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tied charger conversations lasting 45 minutes....</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mic great.</td>\n      <td>pos</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df[\"reviews\"] = df[\"reviews\"].apply(stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:3: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                        way plug us unless go converter\n",
       "1                              good case excellent value\n",
       "2                                          great jawbone\n",
       "3      tied charger conversations lasting 45 minutesm...\n",
       "4                                              mic great\n",
       "                             ...                        \n",
       "995           screen get smudged easily touches ear face\n",
       "996                          piece junk lose calls phone\n",
       "997                                   item match picture\n",
       "998                 thing disappoint infra red port irda\n",
       "999                  answer calls unit never worked once\n",
       "Name: reviews, Length: 1000, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#punctuations\n",
    "import string\n",
    "df['reviews'].str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                     way plug us unless go converter.\n",
       "1                               case, excellent value.\n",
       "2                                             jawbone.\n",
       "3    tied charger conversations lasting 45 minutes....\n",
       "4                                           mic great.\n",
       "Name: reviews, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#remove common words\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"reviews\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)\n",
    "\n",
    "freq = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def freqwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not \n",
    "in freq])\n",
    "df[\"reviews\"] = df[\"reviews\"].apply(freqwords)\n",
    "df[\"reviews\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove emoticons\n",
    "\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(remove_emoticons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\benso\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             reviews sentiment\n",
       "0                    way plug u unless go converter.       neg\n",
       "1                             case, excellent value.       pos\n",
       "2                                           jawbone.       pos\n",
       "3  tied charger conversation lasting 45 minutes.m...       neg\n",
       "4                                         mic great.       pos\n",
       "5      jiggle plug get line right get decent volume.       neg\n",
       "6  several dozen several hundred contacts, imagin...       neg\n",
       "7                        razr owner...you must this!       pos\n",
       "8                          needle say, wasted money.       neg\n",
       "9                                 waste money time!.       neg"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviews</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>way plug u unless go converter.</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>case, excellent value.</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>jawbone.</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tied charger conversation lasting 45 minutes.m...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mic great.</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>jiggle plug get line right get decent volume.</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>several dozen several hundred contacts, imagin...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>razr owner...you must this!</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>needle say, wasted money.</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>waste money time!.</td>\n      <td>neg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#Apply LDA\n",
    "\n",
    "#Lementize\n",
    "\"The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\"\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatizewords(text):\n",
    "    #WordNet Lemmatization\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(lemmatizewords)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 1711)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.01666787, 0.01666667, 0.01666814, ..., 0.01667056, 0.01666682,\n",
       "        0.84997746],\n",
       "       [0.02500157, 0.02500082, 0.02500112, ..., 0.77498438, 0.02500108,\n",
       "        0.02500543],\n",
       "       [0.05      , 0.05      , 0.0500127 , ..., 0.05      , 0.05      ,\n",
       "        0.5499873 ],\n",
       "       ...,\n",
       "       [0.02500359, 0.77497638, 0.02500118, ..., 0.02500257, 0.025     ,\n",
       "        0.025     ],\n",
       "       [0.01428571, 0.01428655, 0.01428606, ..., 0.01428597, 0.87142139,\n",
       "        0.01429075],\n",
       "       [0.01428667, 0.87142076, 0.01428624, ..., 0.01428626, 0.01428759,\n",
       "        0.01428706]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#LDA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_features=5000, max_df=.15)\n",
    "X = vect.fit_transform(df['reviews'])\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\", max_iter=25, random_state=0)\n",
    "\n",
    "document_topics = lda.fit_transform(X)\n",
    "\n",
    "print(lda.components_.shape)\n",
    "document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\n[[ 798 1095  421 ... 1063   23  296]\n [ 671 1220 1208 ...  153  296   23]\n [1101  810  482 ...  728  260 1331]\n ...\n [1156  526 1144 ...   23  153  296]\n [1594  702  482 ... 1063   23  153]\n [ 582  892 1689 ...  296   23   12]]\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "print(len(sorting))\n",
    "print(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1711\n['10' '100' '11' ... 'you' 'z500a' 'zero']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "print(len(feature_names))\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "range(0, 5)\n0 1 2 3 4\n5\ntopic 0       topic 1       topic 2       topic 3       topic 4       \n--------      --------      --------      --------      --------      \nit            great         piece         time          could         \nphone         reception     junk          waste         use           \ndisappointed  really        easy          like          it            \nsamsung       charge        product       money         this          \ntoo           me            fine          ear           well          \nclear         call          it            volume        problem       \nbetter        also          best          product       like          \near           problem       button        work          take          \nprice         worked        sound         audio         make          \ncamera        horrible      ear           case          charger       \n\n\nrange(5, 10)\n5 6 7 8 9\n5\ntopic 5       topic 6       topic 7       topic 8       topic 9       \n--------      --------      --------      --------      --------      \nrecommend     service       product       use           fit           \nitem          ever          excellent     headset       love          \nlife          headset       price         easy          work          \nhighly        worst         well          enough        comfortable   \nnice          customer      quality       looking       case          \ncool          best          great         ear           it            \nlong          ve            happy         bluetooth     well          \nlook          used          case          long          bluetooth     \neven          terrible      work          want          ear           \near           well          working       time          purchase      \n\n\n"
     ]
    }
   ],
   "source": [
    "def print_topics(topics, feature_names, sorting, topics_per_chunk, n_words):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        # for each chunk:\n",
    "        these_topics = topics[i: i + topics_per_chunk]\n",
    "        # maybe we have less than topics_per_chunk left\n",
    "        len_this_chunk = len(these_topics)\n",
    "        print(these_topics)\n",
    "        print(*these_topics)\n",
    "        print(len_this_chunk)\n",
    "        # print topic headers\n",
    "        print((\"topic {:<8}\" * len_this_chunk).format(*these_topics))\n",
    "        print((\"-------- {0:<5}\" * len_this_chunk).format(\"\"))\n",
    "        # print top n_words frequent words\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                print((\"{:<14}\" * len_this_chunk).format(*feature_names[sorting[these_topics, i]]))\n",
    "            except:\n",
    "                pass\n",
    "        print(\"\\n\")\n",
    "        \n",
    "print_topics(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only\n",
    "raw=f.read()\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# remove punctuation, tokenize, and lemmatize in one call\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chatbot\n",
    "\n",
    "#already cleaned the data\n",
    "#already lemmentized\n",
    "\n",
    "# Default greeting messages\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence): \n",
    "    for word in sentence.split():\n",
    "        # If user said hello, greet back\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    \n",
    "    # add user input to sentence tokens\n",
    "    sent_tokens.append(user_response)\n",
    "    # convert sentence tokens to TF-IDF feature matrix [document, word][idf]\n",
    "    tfidfvec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = tfidfvec.fit_transform(sent_tokens)\n",
    "    # calculate cosine similarity between user input to each TF-IDF document (sentence) \n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    # sort cosine similarity values in ascending order, \n",
    "    # then select index of highest cosine similarity value, excluding user input\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    # convert from 2D to 1D array\n",
    "    flat = vals.flatten()\n",
    "    # sort cosine similarity values in ascending order\n",
    "    flat.sort()\n",
    "    # selecting highest cosine similarity, exclusing user input\n",
    "    similarity = flat[-2]\n",
    "    if(similarity==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Amazon reviews. If you want to exit, type 'bye'!\n",
      "C:\\Users\\benso\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "ROBO: how does amazon work?\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Amazon reviews. If you want to exit, type 'bye'!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}